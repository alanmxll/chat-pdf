{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers accelerate gradio langchain pypdf sentence_transformers chromadb llama-cpp-python torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     cache_dir=\"./models/llama-2-7b/\",\n",
    "#     token=\"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\"\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"totally-not-an-llm/AlpacaCielo2-7b-8k\",\n",
    "    cache_dir=\"./models/alpaca_cache/\",\n",
    "    token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     cache_dir=\"./models/llama-2-7b/\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     token=\"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\"\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"totally-not-an-llm/AlpacaCielo2-7b-8k\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=\"./models/alpaca/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./models/pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatPDF:\n",
    "    def __init__(self, files: list, vectordb_path: str) -> None:\n",
    "        self.files = files\n",
    "        self.pages = []\n",
    "        self.documents = []\n",
    "        self.vectordb_path = vectordb_path\n",
    "\n",
    "    def load(self) -> tuple[int, int]:\n",
    "        pages = []\n",
    "\n",
    "        for file in self.files:\n",
    "            print(f\"FILE {file}\")\n",
    "            loader = PyPDFLoader(file)\n",
    "            pages = loader.load()\n",
    "            self.pages.extend(pages)\n",
    "            print(f\"Loading file {file}\")\n",
    "\n",
    "        return len(self.files), len(self.pages)\n",
    "\n",
    "    def split(self, chunk_size: int = 1500, chunk_overlap: int = 150) -> int:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        self.documents = text_splitter.split_documents(self.pages)\n",
    "\n",
    "        return len(self.documents)\n",
    "\n",
    "    def get_embeddings(self) -> None:\n",
    "        self.embeddings = SentenceTransformerEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "    def store(self):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "        )\n",
    "\n",
    "        vectordb.persist()\n",
    "\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def create_llm(self, temperature: float = 0.2) -> None:\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=model.name_or_path,\n",
    "            # model_path=\"/content/models/pretrained/\",\n",
    "            verbose=True,\n",
    "            n_ctx=2048,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def create_memory(self) -> None:\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "        )\n",
    "\n",
    "    def create_retriever(self) -> None:\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def create_chat_session(self):\n",
    "        PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end.\n",
    "        If you don't the answer, just say that you don't know,\n",
    "        don't try to male up the answer.\n",
    "        Use three sentences maximum.\n",
    "        Keep answer as concise as possible.\n",
    "        Always say \"thanks for asking!\" at the end of the answer.\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Helpful Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "        self.qa = ConversationalRetrievalChain.from_llm(\n",
    "            self.llm,\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory,\n",
    "            combine_docs_chain_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"/content/docs/TurboMil F TMF_EN.pdf\"]\n",
    "\n",
    "chat = ChatPDF(files, \"./chroma/\")\n",
    "\n",
    "chat.load()\n",
    "chat.split()\n",
    "chat.get_embeddings()\n",
    "chat.store()\n",
    "chat.create_llm()\n",
    "chat.create_memory()\n",
    "chat.create_retriever()\n",
    "\n",
    "chat.create_chat_session()\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    chat_history = []\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "        result = chat.qa(\n",
    "            {\"question\": user_message, \"chat_history\": chat_history})\n",
    "\n",
    "        chat_history.append((user_message, result[\"answer\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
